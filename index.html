<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title>AI Camera Classifier</title>
  <!-- ‡πÉ‡∏ä‡πâ‡πÄ‡∏ß‡∏≠‡∏£‡πå‡∏ä‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡πÅ‡∏•‡πâ‡∏ß‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏°‡∏µ tmImage -->
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@3.20.0"></script>
  <script src="https://cdn.jsdelivr.net/npm/@teachablemachine/image@0.8.4/dist/teachablemachine-image.min.js"></script>
  <style>
    body { font-family: sans-serif; text-align: center; padding: 20px; }
    video { border-radius: 12px; }
    #label-container > div { margin: 8px; font-size: 18px; }
    button { margin: 10px; padding: 10px 20px; }
  </style>
</head>
<body>
  <h1>üì∑ AI Camera Classifier</h1>
  <button id="start-btn">‚ñ∂Ô∏è Start (Back)</button>
  <button id="switch-btn">üîÑ Switch Camera</button>
  <div><video id="webcam" width="300" height="300" autoplay playsinline></video></div>
  <div id="label-container"></div>

<script>
  const MODEL_PATH = "./tm-my-image-model/";
  let model, maxPredictions, stream;
  let currentFacing = "environment";

  // ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß
  async function loadModel() {
    if (!model) {
      model = await tmImage.load(
        MODEL_PATH + "model.json",
        MODEL_PATH + "metadata.json"
      );
      maxPredictions = model.getTotalClasses();
    }
  }

  // ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏Å‡∏•‡πâ‡∏≠‡∏á‡∏î‡πâ‡∏ß‡∏¢ facingMode
  async function startVideo() {
    try {
      // ‡∏õ‡∏¥‡∏î stream ‡πÄ‡∏Å‡πà‡∏≤‡∏ñ‡πâ‡∏≤‡∏°‡∏µ
      if (stream) {
        stream.getTracks().forEach(t => t.stop());
      }
      // ‡∏ï‡∏±‡πâ‡∏á constraints
      let constraints = { video: { facingMode: { exact: currentFacing }, width: 300, height: 300 }, audio: false };
      try {
        stream = await navigator.mediaDevices.getUserMedia(constraints);
      } catch {
        // fallback ‡πÄ‡∏õ‡πá‡∏ô ideal
        constraints.video.facingMode = currentFacing;
        stream = await navigator.mediaDevices.getUserMedia(constraints);
      }
      document.getElementById("webcam").srcObject = stream;
    } catch (e) {
      alert("‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏Ç‡πâ‡∏≤‡∏ñ‡∏∂‡∏á‡∏Å‡∏•‡πâ‡∏≠‡∏á: " + e.message);
      console.error(e);
    }
  }

  // ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡πÉ‡∏´‡πâ‡∏û‡∏£‡πâ‡∏≠‡∏° ‡πÅ‡∏•‡∏∞‡πÄ‡∏£‡∏¥‡πà‡∏° predict
  async function init() {
    await loadModel();
    await startVideo();
    requestAnimationFrame(loop);
  }

  // ‡∏ß‡∏ô predict
  async function loop() {
    const video = document.getElementById("webcam");
    const predictions = await model.predict(video);
    const container = document.getElementById("label-container");
    container.innerHTML = "";
    predictions.forEach(p => {
      const div = document.createElement("div");
      div.innerText = `${p.className}: ${(p.probability * 100).toFixed(2)}%`;
      container.appendChild(div);
    });
    requestAnimationFrame(loop);
  }

  // ‡∏™‡∏•‡∏±‡∏ö‡∏Å‡∏•‡πâ‡∏≠‡∏á
  async function switchCamera() {
    currentFacing = currentFacing === "environment" ? "user" : "environment";
    await startVideo();
  }

  document.getElementById("start-btn").onclick = init;
  document.getElementById("switch-btn").onclick = switchCamera;
</script>
</body>
</html>
